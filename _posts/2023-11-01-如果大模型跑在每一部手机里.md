---
author: shellc
title: 如果大模型跑在每一部手机里
tags: 技术
---

我们可以预期 LLM 主要应该是运行在 PC 、手机、汽车和其他智能机器的本地。这其中很重要的原因是算力的发展和 LLM 的优化。这同时也解决了隐私和数据安全问题。云上运行 LLM 推理是没有道理的。

<!--more-->

2023 年 10 月 24 日，高通发布骁龙X Elite，采用 4nm 工艺，集成 NPU 提供 45 TOPs 算力，支持运行 13B 参数的 LLM。高通表示其设计初衷是支持未来的高负载智能任务。

2023 年 10 月 31 日，苹果发布 M3 系列芯片，采用 3nm 工艺，其中 M3 Max 提供 16 个神经网络核心，35TOPs 算力。苹果 M2 就已经可以运行LLaMA-2 7B，M3 Max 跑 13B 以上的模型应该没有压力。

这两款 CPU 的出现，预示着一些可能性正在发生。这些可能性以 LLM 带来的新应用场景为基础，带动算力升级，算力升级又催生了新的应用形态的繁荣。

过去 10 年里，除了一些视频和 3D 处理的高负载任务外，PC 算力应该说是严重过剩的。PC 芯片的优化方向是低功耗，而不是高算力。这和 PC 的应用场景有很大关系，过去 10 年 PC 作为生产力工具和游戏娱乐平台，并没有出现新的高负载应用场景。英特尔 i5、i7 其实已经是 10 多年前的产物。直到 2020 年 苹果发布 M1 算是开启了 PC 芯片算力的升级。

回看过去几十年的软硬件发展过程，算力和应用是阶段性相互推动的。最初摩尔定律推动了芯片集成度越来越高，导致了 PC 和图形界面的出现。视频、游戏应用的需求催生了 SSE、GPU 等指令集和专用芯片的产生。移动化对 CPU 的小型化和功耗又提出了更高的要求。当下，AI 应用和 LLM 的爆发可能会导致 CPU 的发展方向出现新的变化。

骁龙X Elite和 M3 Max都可以运行 13B 以上的 LLM，据说 OpenAI GPT-3.5的参数数量也就 20B，我们可以预期在未来一年，PC 芯片可以非常流畅地运行高质量的 LLM 。更远一些的未来，我们可以预期 LLM 主要应该是运行在 PC 、手机、汽车和其他智能机器的本地。这其中很重要的原因是算力的发展和 LLM 的优化。

为什么 LLM 不应该以云计算形态提供。现在说的云计算提供的是零星算力的聚合和弹性供给能力，它很好地解决了互联网应用对于算力的需求。这不意味着云计算是所有应用场景的最优选项。比如大型 3D 多人在线游戏，计算主要发生在客户端，没有人会认为 3D 渲染任务应该在云端完成。LLM 也是一样，无论从成本、体验、隐私和数据安全角度来看，LLM 运行在终端远优于运行在云端。应用需要互联网和计算，但是不一定需要互联网和计算耦合的云计算。云计算更适合数据存储和交易类型的负载，不适合把渲染、人机交互类的计算任务放到云端。LLM 恰恰是这类任务。


如果 LLM 更适合运行在终端，那么 LLM 将会成为操作系统和浏览器的内置能力。我们可以预期未来几年 Window、 MacOS 、 iOS 和 Android会带来几次重大升级，LLM 是每台 PC、Pad和手机的标准配置。同时，PC 和 手机的硬件性能也会得到极大提升。


为 LLM 而升级的硬件，也可以被用来做其他计算，比如图形渲染、科学计算等等，这也会导致其他应用场景的升级和创新，比如更智能和更具沉浸感的元宇宙。甚至大部分商业数据处理之类的任务可能也都可以本地完成了。这就完成了一轮应用推动硬件升级，硬件升级又推动更多应用场景升级的循环。沉浸式元宇宙和Web3也会受益于这轮硬件升级，和 AI 在某个场景中产生完美的融合。

如果大模型跑在每一部手机里，消费级芯片和终端产业生态才是未来的增量市场。赌摩尔定律失效，等待芯片代差逐渐缩小可能不会实现。大规模提前建设数据中心可能也是一种浪费。