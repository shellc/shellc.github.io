---
author: shellc
title: 如果大模型跑在每一台PC里
tags: 技术
---

我们可以预期 LLM 主要应该是运行在 PC 、手机、汽车和其他智能机器的本地。这其中很重要的原因是算力的发展和 LLM 的优化。这同时也解决了隐私和数据安全问题。云上运行 LLM 推理是没有道理的。

<!--more-->

2023 年 10 月 24 日，高通发布骁龙X Elite，采用 4nm 工艺，集成 NPU 提供 45 TOPs 算力，支持运行 13B 参数的 LLM。高通表示其设计初衷是支持未来的高负载智能任务。

2023 年 10 月 31 日，苹果发布 M3 系列芯片，采用 3nm 工艺，其中 M3 Max 提供 16 个神经网络核心，35TOPs 算力。苹果 M2 就已经可以运行LLaMA-2 7B，M3 Max 跑 13B 以上的模型应该没有压力。

这两款 CPU 的出现，可能预示着一些变化会发生了。

除了一些视频和 3D 处理的高负载任务外，PC 算力在过去 10 年应该说是严重过剩的。过去 10 年，PC 芯片的优化方向是低功耗，而不是高算力。这和 PC 的应用场景有很大关系，过去 10 年 PC 作为生产力工具和游戏娱乐平台，并没有出现新的高负载应用场景。英特尔 i5、i7 其实已经是 10 多年前的产物。直到 2020 年 苹果发布 M1 算是开启了 PC 芯片算力的升级。

纵观过去几十年的软硬件发展过程，算力和应用是阶段性相互推动的。最初摩尔定律推动了芯片集成度越来越高，导致了 PC 和图形界面的出现。视频、游戏应用的需求催生了 SSE、GPU 等指令集和专用芯片的产生。移动化对 CPU 的小型化和功耗又提出了更高的要求。当下，AI 应用和 LLM 的爆发可能会导致 CPU 的发展方向出现新的变化。

骁龙X Elite和 M3 Max都可以运行 13B 以上的 LLM，据说 OpenAI GPT-3.5的参数数量也就 20B，我们可以预期在未来一年，PC 芯片可以非常流畅地运行高质量的 LLM 。更远一些的未来，我们可以预期 LLM 主要应该是运行在 PC 、手机、汽车和其他智能机器的本地。这其中很重要的原因是算力的发展和 LLM 的优化。这同时也解决了隐私和数据安全问题。云上运行 LLM 推理是没有道理的。LLM 应该是操作系统或者浏览器内置的能力。未来的本地应用程序或Web应用都可以从各自的应用容器中获得 LLM 能力。未来几年，PC 和手机的算力和内存应该都会有一个数量级的升级。


LLM 改变人机交互界面已经是确定的，接下去的创新方向应该是创造新的应用场景。为 LLM 而升级的硬件，因为也可以被用来做其他计算，比如图形渲染、科学计算等等，这也会导致其他应用场景的升级和创新，比如更智能和更具沉浸感的元宇宙。甚至大部分商业数据处理之类的任务可能也都可以本地完成了。

云计算需求可能会下降。云计算的应用层是互联网负载，一种计算完全在云端的应用形态，这种情况下云计算解决的是算力弹性供给的问题。当用户侧算力提升，计算可能就会往应用侧迁移。应用需要的是互联网和算力，但是不一定需要互联网和算力耦合的云计算。一种极端的形态就是数据和计算都在本地，通过互联网组成点对点网络，这其实和Web3倡导的架构是一致的，目前的大部分应用场景其实都可以被这种架构支持。不过，云端存储可能还是比 IPFS 这种点对点文件系统更有优势。这样的话 AI、元宇宙和Web3 在算力升级的情况下可能会在应用场景上达成某种统一。

国内在 PC、移动端芯片和软硬件生态的差距要比数据中心侧的差距大得多，本来预期摩尔定律减速或终结，让我们有时间消化代差，如果上面的预测成真，我们面临的可能是终端整个软硬件体系的快速落后。更可怕的是，我们为未来的潜在需求建了那么多数据中心。